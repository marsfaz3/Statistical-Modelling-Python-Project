# Final-Project-Statistical-Modelling-with-Python

## Project/Goals
This project used API calls to determine the location of bike stations within a city and to identify points of interest (POIs) around those locations. The goal was to model how features of the pois/API calls may inform the number of free bikes available at each bike station in Toronto.

## Process

1. Explored the webpage for the city bikes to gather information on what it offers and how to make the API call
2. Selected a city to extract data from (Toronto)
3. Followed their documentation for API access and identified the href for Toronto (uses bixi instead of bike share)
4. Extracted bike data in a json and stored it as a dataframe
5. Referred to Foursquare and Yelp documentation to get the codes for identifying pois (decided to focus on restaurants, bars, and cafes). For Foursquare: 13032 (cafe/coffee/tea); 13065 (restaurant); 13003 (bar). For Yelp: used keyword "restaurant" which included cafes and bars. All API calls used a key I stored in .env
6. Used a for loop to run the API calls and extract data from each of the Foursquare and Yelp APIs based on potential shared characteristics to inform the quality of the data within 1000 meters of each bike station (the number of categories to inform how specific they label their data, the number of features/attributes such as payment, the number of results up to a maximum of 50, the average distance of each poi to the bike station lat/long). I limited to 50 stations for simplicity with the dataset
7. Saved a dataframe for the results after extracting them from a .json format
8. Joined the dataframes of the Yelp and Foursquare API calls after examining the features (e.g. shape) of them individually and once combined. Decided against removing duplicate locations between queries (e.g. if station A & B had the same restaurant in their 1000 m radius, which could have been identified with the poi ID) as I felt it would take away from the information I could gather of the quality of data around each query and how that same poi may contribute to the number of free bikes at both of the stations. No duplicates were produced from the API calls though (e.g. two records for Yelp at the same query lat/long due to errors in their dataset)
9. Explored the data using aggregate functions and visualizations to better understand how the Yelp and Foursquare data differed in quality/specificity
10. Created a model assessing the relationship between number of free bikes at the station and the predictors based on the quality of the data for each of Yelp or Foursquare (created a binary variable). Note that all of the variables went into the model but I think the only ones that could potentially be related to the number of free bikes would be the number of pois in the area and the distance (distance was signficant). Even so, the models had poor adjusted R2 values as there may be more factors at play in predicting the number of free bikes than we had available in this analysis

## Results
More complete data initially seems to be provided by Yelp for the criteria that I was interested in extracting. I wanted to collect the distance from the points of interest (pois) to the query lat/long to inform how far the pois available from each API were to the query site. I also thought that the number of features available and number of categories used to classify each site could inform how detailed the information was from each API. Interestingly, none of the features from the FourSquare data were available for this set of restaurants/cafes/bars in Toronto. I thought it may be a issue with how I ran the API call initially as some resources only mentioned getting features from the call using a specific site ID, so I reran my code to embed an API call using the site ID but still was not able to gather any features. After spot-checking some of the json outputs I came to the conclusion that there were no features available for the pois in this dataset using the free version of Foursquare. Therefore, the Yelp API provided more complete data.

Overall, Yelp maxed out at 50 pois per request (maximum allowed was 50), whereas Foursquare was not able to offer 50 pois for many requests at the same lat/long coordinates. Of note, for the Yelp data I indicated a maximum radius of 1000 meters, but the data indicates a distance up to 2000 meters. While I initially thought that the Yelp API may have not abided to my radius limits, I suspect that the Yelp distance is calculated based on travel distance along set paths but the Foursqaure distance seems to be calculated based on straight paths between the lat/long of the query and poi. This may create some difficulty when assessing the relationship between average poi distance and number of free bikes in the model.

Based on the adjusted R2 of this model (0.064), the initial model was not a good model to predict the number of free bikes based on the presence and quality of information on restaurants, bars, or cafes nearby. The only signficant variable in the model was the average distance between pois and the bike station, and the coefficient suggests that as average distance increases the number of free bikes decreases. However, if you refer to my discussion in the joining_data file, I mention that distance may represent different things in the Yelp or Foursquare data (straight-line verus path-analysis), which may impact findings. I re-ran the model with only distance (and I kept the source variable as I felt excluding it could also bias the results as you would have two data points per query long/lat) and the adjusted R2 did not improve much.

Of note, I used aggregate calculations per query as I felt that it better reflected the quality of data per query around each bike station and wanted to avoid potential correlation where each bike station had multiple data points that may not have been independent. Also I did not remove duplicates between queries (e.g. if station A & B had the same restaurant in their 1000 m radius, which could have been identified with the poi ID) as I felt it would take away from the information I could gather of the quality of data around each query and how that same poi may contribute to the number of free bikes at both of the stations. 

In my opinion, these results suggest that there may be more factors contributing to the number of free bikes at each bike station in Toronto than the presence of different restaurants, bars, or cafes in the area. I live in Toronto and from experience can say that certain neighbourhoods are frequented by different types of people who may not enjoy commuting with bikes to go to these kinds of establishments, whereas other neighbourhoods or cities may have different patterns (e.g. in Montreal I know many locals who would enjoy biking home after going to the bar with friends). 

## Challenges 
I faced the challenge of identifying features in the Foursquare API call and trouble-shooting to try to determine if the issue was due to errors in my coding or limited data produced by the API call itself. I used ChatGPT and Stackoverflow to try and figure out what might have caused issues wih the call but based on my testing of the unedited .json files extracted for each call, Foursquare was not providing this information for the listed bike stations. I also found it to be a (fun) challenge problem-solving how to run multiple API calls for numerous lat/long coordinates and extract the data as efficiently as possible, which I solved using a for-loop. 

## Future Goals
If I had more time I would re-do the data extraction with each API service to extract for all 800 bike stations in order to have more data for the analysis. I would also try to investigate and extract items from each API call to see if they contribute to the quality of the data and could fit a better model. I would also be interested to compare between different cities with different biking cultures (e.g. Toronto versus Amsterdam).
